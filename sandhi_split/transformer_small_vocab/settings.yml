# Transformer configuration for single GPU training.
model_dir: model_dir

params:
  optimizer: LazyAdamOptimizer
  learning_rate: 2.0  # The scale constant.
  decay_type: noam_decay
  decay_rate: 512  # Model dimension.
  decay_steps: 4000  # Warmup steps.

  # Divide this value by the total number of GPUs used.
  decay_step_duration: 8  # 1 decay step is 8 training steps.

  average_loss_in_time: true
  label_smoothing: 0.1

  beam_width: 4
  length_penalty: 0.6

train:
  batch_size: 3072
  batch_type: tokens
  bucket_width: 1

  maximum_features_length: 100
  maximum_labels_length: 100

  save_checkpoints_steps: 500
  keep_checkpoint_max: 8
  save_summary_steps: 100
  train_steps: 10000

  # Consider setting this to -1 to match the number of training examples.
  sample_buffer_size: -1

eval:
  batch_size: 128
  eval_delay: 600  # Every 10 mins
  external_evaluators: BLEU

infer:
  batch_size: 32
