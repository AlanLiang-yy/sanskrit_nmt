#!/bin/bash

vocab_size=8000
model=sandhi_split

# Data preparation using SentencePiece
# First we concat all the training data to train the SP model
echo "$0: Training sentencepiece model"
cat input_train.txt >> train.txt
cat output_train.txt >> train.txt
spm_train --input=train.txt --model_prefix=$model \
           --vocab_size=$vocab_size --character_coverage=1
rm train.txt

# Second we use the trained model to tokenize all the files
echo "$0: Tokenizing with sentencepiece model"
spm_encode --model=$model.model < input_train.txt > input_train_sp.txt
spm_encode --model=$model.model < output_train.txt > output_train_sp.txt

#  We use the same tokenization method for a valid set (and test set)
spm_encode --model=$model.model < input_test.txt > input_test_sp.txt
spm_encode --model=$model.model < output_test.txt > output_test_sp.txt

# Let's finish and clean up
# We keep the first field of the vocab file generated by SentencePiece and remove the first line <unk>
cut -f 1 $model.vocab | tail -n +2 > $model.vocab.tmp
# we add the <blank> word in first position, needed for OpenNMT-TF
sed -i '1i<blank>' $model.vocab.tmp
# Last tweak we replace the empty line supposed to be the "tab" character (removed by the cut above)
perl -pe '$/=""; s/\n\n/\n\t\n/;' $model.vocab.tmp > ${model}_sp.vocab
rm $model.vocab.tmp
